{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mechanisms of Action (MoA) Prediction Project: \n",
    "#### Data Source: https://www.kaggle.com/competitions/lish-moa/overview/description\n",
    "#### Code Generated By: Mark D. Espinoza\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![picture 3](../images/21ad447f2d79344afabc719d92e5f9c98d6b025703213f3d3b8e65313116fbdf.png)\n",
    "\n",
    "> Image 1: Women holding a drug with the appearance of analyzing it visually. \n",
    "\n",
    "Scientist are consistently working on creating new drug products, but the process of developing new drugs has significantly improved with the advent of new and more powerful technologies. Drug discovery has changed in that it is approached to target specific mechanism of a disease. Which is to identify a protein target associated with a disease and then develop a drug that is able to modulate the protein, thus reducing or eliminating the disease. \n",
    "\n",
    "Scientist describe the activity of a drug as mechanism-of-action, which entails the overall outcome the drug has on the targeted protein and other types of cells. \n",
    "\n",
    "\n",
    "### How Scientist determine the MOA of a drug? \n",
    "\n",
    "Treat human cells with a drug and the monitor and analyze the cellular responses with algorithms that search for similar patterns on large genomic databases. \n",
    "\n",
    "### Dataset Description\n",
    "\n",
    "The dataset is a combination of gene expression and cell viability data. The data is from a new technology that measures simultaneously human cells' responses to drugs in a pool of 100 different cell types. Also the data has annotations for more than 5,000 drugs in that dataset. \n",
    "\n",
    ">Note: The data data is already split into training and testing subsets. Also, the goal is define the drug/molecule into one or more MOA classes (\"multi-label classification problem\")\n",
    "\n",
    "\n",
    "### Objectives for this project\n",
    "\n",
    "- The aim of this project is to develop and fit the best model for the predication of the best response to a specific target (gene expression and cell viability).\n",
    "\n",
    "Other Questions/Insights from the data:\n",
    "- What is the overall biological activity of a given molecule/drug? (Does it impact other cell type?)\n",
    "- Are there any cell types that are always affected?\n",
    "- Are there drugs that are intended for a specific cell type but have equal or greater effects on other cell types?\n",
    "\n",
    "\n",
    "sources: \n",
    "- https://www.kaggle.com/competitions/lish-moa/overview/description\n",
    "\n",
    "---\n",
    "author:{Jin Paik, Maggie, mrbhbs, Steven Randazzo , tnat1031},\n",
    "title:{Mechanisms of Action (MoA) Prediction},\n",
    "publisher:{Kaggle},\n",
    "year:{2020},\n",
    "url:{https://kaggle.com/competitions/lish-moa}\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Literature Review:\n",
    "\n",
    "![picture 4](../images/9461f0d4f4cfe03e5a435fd024a6aa9447ca6a1166d7f47f647e8d03479b37a0.png)\n",
    "\n",
    "> Image 2: First - Publication regarding drug modeling: Multiscale modelling of drug mechanism and safety\n",
    "\n",
    "Summary of the Article: \n",
    "A drug discovery program is setup to maximize the full knowledge of a molecule prior to every being tested/given to a human. There is a full understanding of a new drug in vitro and in vivo. There are many factors that are studied beyond just the MOA of a drug such as Negative Feedback regulation of a specific target. Additionally it is noted that drugs may have interactions with multiple targets, with varying affinities (affinity is related on how a molecule reaction/interaction/acceptance).\n",
    "Modern drug discovery programs focus on targeting specific targets which have lead to the following examples of specific domains in drug development: \n",
    "1. Structure-based molecular modeling \n",
    "2. Ligand Based molecular modeling \n",
    "3. Modeling with omnics data\n",
    "\n",
    "\n",
    "In summary drug development is increasingly becoming more multiscale modeling combining models from the initial determination of a target to the overall effect it has on a population. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![picture 7](../images/67f626c53eca37b85b4af52e543b51287d938facd72e89086d12841e8c5d7a32.png)  \n",
    "\n",
    "> Image 3: Second - Publication: Modeling drug mechanism of action with large scale gene‐expression profiles using GPAR, an artificial intelligence platform\n",
    "\n",
    "Summary of the Article: \n",
    "With the advent of large drug-induced gene expression profiles machine learning/predictive modeling methods is an effective methods for discovering the MOA of drugs. However there is a lack of adoption by people since there is a lack in open source platforms/code and user friendly platforms. Overall they developed a GPAR method and online tool to connect MOA with gene expression signatures, providing a simple and effective deep learning-based modeling and prediction method for drug researchers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![picture 8](../images/f7e9c889247abf4cb83a93b9b86202e1256fc598800ca58976246ec01d28f5fc.png)  \n",
    "\n",
    "> Image 4: Third - Publication: Mechanism-Based Pharmacodynamic Modeling\n",
    "\n",
    "Summary of the Article: \n",
    "The modeling of drug development is promising and many of the modeling methods described in the article will be the start of more complex models. \n",
    "Types of Models Mentioned: \n",
    "- Simple Direct Effective Models: (Hill Equation: Represents a fundamental pharmacodynamic relationship)\n",
    "- Biophase Distribution: In reference to plasma drug concentrations and time to reach target and the onset of side-effects\n",
    "- Indirect Response Models: Reversible drug receptor interactions serve to alter the natural production or loss of biomarker response variables \n",
    "- and more... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriptive Analysis of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import time\n",
    "from varclushi import VarClusHi\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.decomposition import PCA \n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss \n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from skmultilearn.model_selection import iterative_train_test_split\n",
    "from skmultilearn.model_selection import IterativeStratification\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "from sklearn.svm import SVC\n",
    "import random\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import xgboost\n",
    "from xgboost import XGBClassifier\n",
    "%matplotlib inline\n",
    "\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import Data and view the dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- train_features.csv - Features for the training set. Features g- signify gene expression data, and c- signify cell viability data. cp_type indicates samples treated with a compound (cp_vehicle) or with a control perturbation (ctrl_vehicle); control perturbations have no MoAs; cp_time and cp_dose indicate treatment duration (24, 48, 72 hours) and dose (high or low).\n",
    "- train_drug.csv - This file contains an anonymous drug_id for the training set only.\n",
    "- train_targets_scored.csv - The binary MoA targets that are scored.\n",
    "- train_targets_nonscored.csv - Additional (optional) binary MoA responses for the training data. These are not predicted nor scored.\n",
    "- test_features.csv - Features for the test data. You must predict the probability of each scored MoA for each row in the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import both the data\n",
    "train_features = pd.read_csv(\"/Users/mdespinoza/Desktop/Github/data/train_features.csv\")\n",
    "test_features = pd.read_csv(\"/Users/mdespinoza/Desktop/Github/data/test_features.csv\")\n",
    "test_features['flag']='test'\n",
    "train_features['flag']='train'\n",
    "train_drug = pd.read_csv(\"/Users/mdespinoza/Desktop/Github/data/train_drug.csv\")\n",
    "train_targets_nonscored = pd.read_csv(\"/Users/mdespinoza/Desktop/Github/data/train_targets_nonscored.csv\")\n",
    "train_targets_scored = pd.read_csv(\"/Users/mdespinoza/Desktop/Github/data/train_targets_scored.csv\")\n",
    "f=train_features.merge(train_drug, how='left', on='sig_id')\n",
    "\n",
    "merged=pd.concat([train_features,test_features], ignore_index=True, sort=False)\n",
    "\n",
    "train_features_scored=pd.merge(train_features,train_targets_scored,how='inner')\n",
    "\n",
    "#Datasets for treated and control experiments\n",
    "treated= train_features[train_features['cp_type']=='trt_cp']\n",
    "control= train_features[train_features['cp_type']=='ctl_vehicle']\n",
    "\n",
    "#Datasets for treated and control: TEST SET\n",
    "treated_t= test_features[test_features['cp_type']=='trt_cp']\n",
    "control_t= test_features[test_features['cp_type']=='ctl_vehicle']\n",
    "\n",
    "#Treatment time datasets\n",
    "cp24= train_features[train_features['cp_time']== 24]\n",
    "cp48= train_features[train_features['cp_time']== 48]\n",
    "cp72= train_features[train_features['cp_time']== 72]\n",
    "\n",
    "#Merge scored and nonscored labels\n",
    "all_drugs= pd.merge(train_targets_scored, train_targets_nonscored, on='sig_id', how='inner')\n",
    "\n",
    "#Treated drugs without control\n",
    "treated_list = treated['sig_id'].to_list()\n",
    "drugs_tr= train_targets_scored[train_targets_scored['sig_id'].isin(treated_list)]\n",
    "\n",
    "#Select the columns c-\n",
    "c_cols3 = [col for col in test_features.columns if 'c-' in col]\n",
    "#Filter the TEST set\n",
    "cells3=treated[c_cols3]\n",
    "\n",
    "\n",
    "#view that all the data is there by seeing the shape \n",
    "print(train_features.shape)\n",
    "print(test_features.shape)\n",
    "print(train_drug.shape)\n",
    "print(train_targets_nonscored.shape)\n",
    "print(train_targets_scored.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_drug.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_targets_nonscored.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_targets_scored.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visualizations of training dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Treatment distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treatment_group = train_features.groupby(['cp_type'])['sig_id'].count().reset_index()\n",
    "\n",
    "treatment_group.columns = [\n",
    "    'cp_type',  \n",
    "    'count'\n",
    "]\n",
    "\n",
    "bar_plot_cp_type = px.bar(\n",
    "    treatment_group, \n",
    "    x='cp_type', \n",
    "    y=\"count\", \n",
    "    barmode='group',\n",
    "    orientation='v', \n",
    "    title='cp_type',\n",
    "    text_auto='.2s'\n",
    ")\n",
    "bar_plot_cp_type.update_layout(title_x=0.5)\n",
    "\n",
    "bar_plot_cp_type.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Distribution of treatment type - which is not balanced. Note there are few controls in contrast to treated. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dose distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_group = train_features.groupby(['cp_time'])['sig_id'].count().reset_index()\n",
    "\n",
    "time_group.columns = [\n",
    "    'cp_time', \n",
    "    'count'\n",
    "]\n",
    "\n",
    "time_plot = px.bar(\n",
    "    time_group, \n",
    "    x='cp_time', \n",
    "    y=\"count\",\n",
    "    orientation='v',\n",
    "    title='cp_time',\n",
    "    text_auto='.2s'\n",
    ")\n",
    "\n",
    "time_plot.update_layout(title_x=0.5)\n",
    "\n",
    "time_plot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Time distribution of when drugs were provided and it is distributed more evenly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "sns.distplot( train_features['cp_time'], color='red', bins=5)\n",
    "plt.title(\"Train: Treatment duration \", fontsize=15, weight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Plot showing the time distribution of when drugs were given"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Duration distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dose_group = train_features.groupby(['cp_dose'])['sig_id'].count().reset_index()\n",
    "\n",
    "dose_group.columns = [\n",
    "    'cp_dose', \n",
    "    'count'\n",
    "]\n",
    "\n",
    "does_plot = px.bar(\n",
    "    dose_group, \n",
    "    x='cp_dose', \n",
    "    y=\"count\",\n",
    "    orientation='v',\n",
    "    title='cp_dose',\n",
    "    text_auto='.2s'\n",
    ")\n",
    "\n",
    "does_plot.update_layout(title_x=0.5)\n",
    "\n",
    "does_plot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The does distribution of low versus high even. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cell Viability\n",
    "Cell Viability is defined as the number of healthy cells in a sample and proliferation of cells is a vital indicator for understanding the mechanisms in action of certain genes, proteins and pathways involved cell survival or death after exposing to toxic agents. \n",
    "\n",
    "sources: \n",
    "1. https://pubmed.ncbi.nlm.nih.gov/27604355/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotf(f1, f2, f3, f4):\n",
    "    plt.style.use('seaborn')\n",
    "    sns.set_style('whitegrid')\n",
    "\n",
    "    fig= plt.figure(figsize=(15,10))\n",
    "    #2 rows 2 cols\n",
    "    #first row, first col\n",
    "    ax1 = plt.subplot2grid((2,2),(0,0))\n",
    "    sns.distplot(train_features[f1], color='crimson')\n",
    "    plt.title(f1,weight='bold', fontsize=18)\n",
    "    plt.yticks(weight='bold')\n",
    "    plt.xticks(weight='bold')\n",
    "    #first row sec col\n",
    "    ax1 = plt.subplot2grid((2,2), (0, 1))\n",
    "    sns.distplot(train_features[f2], color='gainsboro')\n",
    "    plt.title(f2,weight='bold', fontsize=18)\n",
    "    plt.yticks(weight='bold')\n",
    "    plt.xticks(weight='bold')\n",
    "    #Second row first column\n",
    "    ax1 = plt.subplot2grid((2,2), (1, 0))\n",
    "    sns.distplot(train_features[f3], color='deepskyblue')\n",
    "    plt.title(f3,weight='bold', fontsize=18)\n",
    "    plt.yticks(weight='bold')\n",
    "    plt.xticks(weight='bold')\n",
    "    #second row second column\n",
    "    ax1 = plt.subplot2grid((2,2), (1, 1))\n",
    "    sns.distplot(train_features[f4], color='black')\n",
    "    plt.title(f4,weight='bold', fontsize=18)\n",
    "    plt.yticks(weight='bold')\n",
    "    plt.xticks(weight='bold')\n",
    "\n",
    "    return plt.show()\n",
    "\n",
    "\n",
    "def plotd(f1):\n",
    "    plt.style.use('seaborn')\n",
    "    sns.set_style('whitegrid')\n",
    "    fig = plt.figure(figsize=(15,5))\n",
    "    #1 rows 2 cols\n",
    "    #first row, first col\n",
    "    ax1 = plt.subplot2grid((1,2),(0,0))\n",
    "    plt.hist(control[f1], bins=4, color='mediumpurple',alpha=0.5)\n",
    "    plt.title(f'control: {f1}',weight='bold', fontsize=18)\n",
    "    #first row sec col\n",
    "    ax1 = plt.subplot2grid((1,2),(0,1))\n",
    "    plt.hist(treated[f1], bins=4, color='darkcyan',alpha=0.5)\n",
    "    plt.title(f'Treated with drugs: {f1}',weight='bold', fontsize=18)\n",
    "    plt.show()\n",
    "    \n",
    "def plott(f1):\n",
    "    plt.style.use('seaborn')\n",
    "    sns.set_style('whitegrid')\n",
    "    fig = plt.figure(figsize=(15,5))\n",
    "    #1 rows 2 cols\n",
    "    #first row, first col\n",
    "    ax1 = plt.subplot2grid((1,3),(0,0))\n",
    "    plt.hist(cp24[f1], bins=3, color='deepskyblue',alpha=0.5)\n",
    "    plt.title(f'Treatment duration 24h: {f1}',weight='bold', fontsize=14)\n",
    "    #first row sec col\n",
    "    ax1 = plt.subplot2grid((1,3),(0,1))\n",
    "    plt.hist(cp48[f1], bins=3, color='lightgreen',alpha=0.5)\n",
    "    plt.title(f'Treatment duration 48h: {f1}',weight='bold', fontsize=14)\n",
    "    #first row 3rd column\n",
    "    ax1 = plt.subplot2grid((1,3),(0,2))\n",
    "    plt.hist(cp72[f1], bins=3, color='gold',alpha=0.5)\n",
    "    plt.title(f'Treatment duration 72h: {f1}',weight='bold', fontsize=14)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def ploth(data, w=15, h=9):\n",
    "    plt.figure(figsize=(w,h))\n",
    "    sns.heatmap(data.corr(), cmap='hot')\n",
    "    plt.title('Correlation between targets', fontsize=25, weight='bold')\n",
    "    return plt.show()\n",
    "\n",
    "# corrs function: Show dataframe of high correlation between features\n",
    "def corrs(data, col1='Gene 1', col2='Gene 2',rows=5,thresh=0.8, pos=[1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53]):\n",
    "        #Correlation between genes\n",
    "        corre= data.corr()\n",
    "         #Unstack the dataframe\n",
    "        s = corre.unstack()\n",
    "        so = s.sort_values(kind=\"quicksort\", ascending=False)\n",
    "        #Create new dataframe\n",
    "        so2= pd.DataFrame(so).reset_index()\n",
    "        so2= so2.rename(columns={0: 'correlation', 'level_0':col1, 'level_1': col2})\n",
    "        #Filter out the coef 1 correlation between the same drugs\n",
    "        so2= so2[so2['correlation'] != 1]\n",
    "        #Drop pair duplicates\n",
    "        so2= so2.reset_index()\n",
    "        pos = pos\n",
    "        so3= so2.drop(so2.index[pos])\n",
    "        so3= so3.drop('index', axis=1)\n",
    "        #Show the first 10 high correlations\n",
    "        cm = sns.light_palette(\"Red\", as_cmap=True)\n",
    "        s = so3.head(rows).style.background_gradient(cmap=cm)\n",
    "        print(f\"{len(so2[so2['correlation']>thresh])/2} {col1} pairs have +{thresh} correlation.\")\n",
    "        return s\n",
    "\n",
    "\n",
    "def plotgene(data):\n",
    "    sns.set_style('whitegrid')    \n",
    "    data.plot.bar(color=sns.color_palette('Reds',885), edgecolor='black')\n",
    "    set_size(13,5)\n",
    "    #plt.xticks(rotation=90)\n",
    "    plt.tick_params(\n",
    "        axis='x',          # changes apply to the x-axis\n",
    "        which='both',      # both major and minor ticks are affected\n",
    "        bottom=False,      # ticks along the bottom edge are off\n",
    "        top=False,         # ticks along the top edge are off\n",
    "        labelbottom=False) # labels along the bottom edge are off\n",
    "    plt.ylabel('Gene expression values', weight='bold')\n",
    "    plt.title('Mean gene expression of the 772 genes', fontsize=15)\n",
    "    return plt.show()\n",
    "\n",
    "def mean(row):\n",
    "    return row.mean()\n",
    "\n",
    "def set_size(w,h, ax=None):\n",
    "    \"\"\" w, h: width, height in inches \"\"\"\n",
    "    if not ax: ax=plt.gca()\n",
    "    l = ax.figure.subplotpars.left\n",
    "    r = ax.figure.subplotpars.right\n",
    "    t = ax.figure.subplotpars.top\n",
    "    b = ax.figure.subplotpars.bottom\n",
    "    figw = float(w)/(r-l)\n",
    "    figh = float(h)/(t-b)\n",
    "    ax.figure.set_size_inches(figw, figh)\n",
    "    \n",
    "def unidrug(data):\n",
    "    #Filter out just the treated samples\n",
    "    scored= data[data['sig_id'].isin(treated_list)]\n",
    "\n",
    "    #Count unique values per column\n",
    "    cols = data.columns.to_list() # specify the columns whose unique values you want here\n",
    "    uniques = {col: data[col].nunique() for col in cols}\n",
    "    uniques=pd.DataFrame(uniques, index=[0]).T\n",
    "    uniques=uniques.rename(columns={0:'count'})\n",
    "    uniques= uniques.drop('sig_id', axis=0)\n",
    "    return uniques\n",
    "\n",
    "def avgdrug(data):\n",
    "    \n",
    "    uniques=unidrug(data)\n",
    "\n",
    "     #Calculate the mean values\n",
    "    scored= data[data['sig_id'].isin(treated_list)]\n",
    "    average=scored.mean()\n",
    "    average=pd.DataFrame(average)\n",
    "    average=average.rename(columns={ 0: 'mean'})\n",
    "    average['percentage']= average['mean']*100\n",
    "    \n",
    "    return average\n",
    "\n",
    "def avgfiltered(data):\n",
    "    \n",
    "    average= avgdrug(data)\n",
    "    #Filter just the drugs with mean >0.01\n",
    "    average_filtered= average[average['mean'] > 0.01]\n",
    "    average_filtered= average_filtered.reset_index()\n",
    "    average_filtered= average_filtered.rename(columns={'index': 'drug'})\n",
    "    return average_filtered\n",
    "\n",
    "def plotc(data, column, width=10, height=6, color=('silver', 'gold','lightgreen','skyblue','lightpink'), edgecolor='black'):\n",
    "    \n",
    "        fig, ax = plt.subplots(figsize=(width,height))\n",
    "        title_cnt=data[column].value_counts()[:15].sort_values(ascending=True).reset_index()\n",
    "        mn= ax.barh(title_cnt.iloc[:,0], title_cnt.iloc[:,1], color=sns.color_palette('Reds',len(title_cnt)))\n",
    "\n",
    "        tightout= 0.008*max(title_cnt[column])\n",
    "        ax.set_title(f'Count of {column}', fontsize=15, weight='bold' )\n",
    "        ax.set_ylabel(f\"{column}\", weight='bold', fontsize=12)\n",
    "        ax.set_xlabel('Count', weight='bold')\n",
    "        if len(data[column].unique()) < 17:\n",
    "            plt.xticks(rotation=65)\n",
    "        else:\n",
    "            plt.xticks(rotation=90)\n",
    "        for i in ax.patches:\n",
    "            ax.text(i.get_width()+ tightout, i.get_y()+0.1, str(round((i.get_width()), 2)),\n",
    "             fontsize=10, fontweight='bold', color='grey')\n",
    "        return\n",
    "    \n",
    "def plot_drugid(drug_id):\n",
    "    g=d[f['drug_id']==drug_id]\n",
    "    average_filtered2=avgfiltered(g)\n",
    "\n",
    "    plt.figure(figsize=(5,2))\n",
    "    average_filtered2.sort_values('percentage', inplace=True) \n",
    "    plt.scatter(average_filtered2['percentage'], average_filtered2['drug'], color=sns.color_palette('Reds',len(average_filtered2)))\n",
    "    plt.title(f'Targets with higher presence in the drug: {drug_id} ', weight='bold', fontsize=15)\n",
    "    plt.xticks(weight='bold')\n",
    "    plt.yticks(weight='bold')\n",
    "    plt.xlabel('Percentage', fontsize=13)\n",
    "    return plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the cell-viability\n",
    "plotf('c-10', 'c-18', 'c-72', 'c-95')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Cell viability is graphed but note that is the value is more positive then that is an indication of a thriving/living cell, while negative means that the cells are dying/not well.\n",
    "> If a cell is treated with a drug the expectation that the cell viability will decrease if it is the target cell.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotd(\"c-75\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Difference in control versus treated. As noted the treated should be more negative if it is the target cell for the new drug."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plott(\"c-75\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> These plot show the overall effect of the treatment on a cell from 24 hr - 72 hr."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the columns c-\n",
    "c_cols = [col for col in train_features.columns if 'c-' in col]\n",
    "#Filter the columns c-\n",
    "cells=treated[c_cols]\n",
    "#Plot heatmap\n",
    "plt.figure(figsize=(15,6))\n",
    "sns.heatmap(cells.corr(), cmap='coolwarm', alpha=0.9)\n",
    "plt.title('Correlation: Cell viability', fontsize=15, weight='bold')\n",
    "plt.xticks(weight='bold')\n",
    "plt.yticks(weight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrs(cells, 'Cell', 'Cell 2', rows=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gene Expression\n",
    "Gene expression is the process by which information from a gene is used in the synthesis of a functional gene product that enables it to produce end products, protein or non-coding RNA, and ultimately affect a phenotype, as the final effect.\n",
    "\n",
    "sources: \n",
    "1. https://en.wikipedia.org/wiki/Gene_expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotf('g-10','g-100','g-200','g-400')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Gene Expression plot showing the distribution of gene expression. (Positive = expressed, Negative = Not Expressed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotd('g-510')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Treated versus not Treated expression. (Here is can be seen that the expression has increased for the gene) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plott('g-510')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> View the gene expression throughout time and seeing the impact for suppression at 48h. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the columns g-\n",
    "g_cols = [col for col in train_features.columns if 'g-' in col]\n",
    "#Filter the columns g-\n",
    "genes=treated[g_cols]\n",
    "#Plot heatmap\n",
    "# plt.figure(figsize=(15,7))\n",
    "plt.figure(figsize=(15,6))\n",
    "sns.heatmap(genes.corr(), cmap='coolwarm', alpha=0.9)\n",
    "plt.title('Gene expression: Correlation', fontsize=15, weight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Correlation of gene expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrs(genes, 'Gene', 'Gene 2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correlation between drugs\n",
    "corre= genes.corr()\n",
    "#Unstack the dataframe\n",
    "s = corre.unstack()\n",
    "so = s.sort_values(kind=\"quicksort\", ascending=False)\n",
    "#Create new dataframe\n",
    "so2= pd.DataFrame(so).reset_index()\n",
    "so2= so2.rename(columns={0: 'correlation', 'level_0':'Drug 1', 'level_1': 'Drug2'})\n",
    "#Filter out the coef 1 correlation between the same drugs\n",
    "so2= so2[so2['correlation'] != 1]\n",
    "#Drop pair duplicates\n",
    "so2= so2.reset_index()\n",
    "so2= so2.sort_values(by=['correlation'])\n",
    "pos = [1,3,5,7,9,11,13,15,17,19,21]\n",
    "so2= so2.drop(so2.index[pos])\n",
    "so2= so2.round(decimals=4)\n",
    "so2=so2.drop('index', axis=1)\n",
    "so3=so2.head(4)\n",
    "#Show the first 10 high correlations\n",
    "cm = sns.light_palette(\"Red\", as_cmap=True)\n",
    "s = so2.head().style.background_gradient(cmap=cm)\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 34 genes that have a high pair correlation with other genes\n",
    "It should also be noted that there are both positive and negative correlations - this implying that some drugs may be designed to increase the expression of a gene (upregulate/downregulate)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MOA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average= avgdrug(train_targets_scored)\n",
    "uniques=unidrug(train_targets_scored)\n",
    "average_filtered=avgfiltered(train_targets_scored)\n",
    "\n",
    "plt.style.use('seaborn')\n",
    "sns.set_style('whitegrid')\n",
    "fig = plt.figure(figsize=(15,5))\n",
    "#1 rows 2 cols\n",
    "#first row, first col\n",
    "ax1 = plt.subplot2grid((1,2),(0,0))\n",
    "sns.countplot(uniques['count'], color='deepskyblue', alpha=0.75)\n",
    "plt.title('Unique elements per target [0,1]', fontsize=15, weight='bold')\n",
    "#first row sec col\n",
    "ax1 = plt.subplot2grid((1,2),(0,1))\n",
    "sns.distplot(average['percentage'], color='orange', bins=20)\n",
    "plt.title(\"The targets mean distribution\", fontsize=15, weight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> All targets can be found in the samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,7))\n",
    "average_filtered.sort_values('percentage', inplace=True) \n",
    "plt.scatter(average_filtered['percentage'], average_filtered['drug'], color=sns.color_palette('Reds',len(average_filtered)))\n",
    "plt.title('Targets with higher presence in train samples', weight='bold', fontsize=15)\n",
    "plt.xticks(weight='bold')\n",
    "plt.yticks(weight='bold')\n",
    "plt.xlabel('Percentage', fontsize=13)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Distribution of types of labels in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inhibitors = [col for col in train_targets_scored.columns if 'inhibitor' in col]\n",
    "activators = [col for col in train_targets_scored.columns if 'activator' in col]\n",
    "antagonists = [col for col in train_targets_scored.columns if 'antagonist' in col]\n",
    "agonists = [col for col in train_targets_scored.columns if 'agonist' in col]\n",
    "modulators = [col for col in train_targets_scored.columns if 'modulator' in col]\n",
    "receptors = [col for col in train_targets_scored.columns if 'receptor' in col]\n",
    "receptors_ago = [col for col in train_targets_scored.columns if 'receptor_agonist' in col]\n",
    "receptors_anta = [col for col in train_targets_scored.columns if 'receptor_antagonist' in col]\n",
    "\n",
    "\n",
    "labelss= {'Drugs': ['inhibitors', 'activators', 'antagonists', 'agonists', 'receptors', 'receptors_ago', 'receptors_anta'],\n",
    "          'Count':[112,5,32,60, 53, 24, 26]}\n",
    "\n",
    "\n",
    "labels= pd.DataFrame(labelss)\n",
    "labels=labels.sort_values(by=['Count'])\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.bar(labels['Drugs'], labels['Count'], color=sns.color_palette('Reds',len(labels)))\n",
    "plt.xticks(weight='bold')\n",
    "plt.title('Target types', weight='bold', fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The majority of targets are inhibitors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ploth(drugs_tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Correlation matrix: Showing there are very few strong correlations with targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correlation between drugs\n",
    "corre= drugs_tr.corr()\n",
    "#Unstack the dataframe\n",
    "s = corre.unstack()\n",
    "so = s.sort_values(kind=\"quicksort\", ascending=False)\n",
    "#Create new dataframe\n",
    "so2= pd.DataFrame(so).reset_index()\n",
    "so2= so2.rename(columns={0: 'correlation', 'level_0':'Target 1', 'level_1': 'Target 2'})\n",
    "#Filter out the coef 1 correlation between the same drugs\n",
    "so2= so2[so2['correlation'] != 1]\n",
    "#Drop pair duplicates\n",
    "so2= so2.reset_index()\n",
    "pos = [1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35]\n",
    "so2= so2.drop(so2.index[pos])\n",
    "so2= so2.round(decimals=4)\n",
    "so2=so2.drop('index', axis=1)\n",
    "so3=so2.head(4)\n",
    "#Show the first 10 high correlations\n",
    "cm = sns.light_palette(\"Red\", as_cmap=True)\n",
    "s = so2.head().style.background_gradient(cmap=cm)\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Targets with the highest MOA correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,10))\n",
    "the_table =plt.table(cellText=so3.values,colWidths = [0.35]*len(so3.columns),\n",
    "          rowLabels=so3.index,\n",
    "          colLabels=so3.columns\n",
    "          ,cellLoc = 'center', rowLoc = 'center',\n",
    "          loc='left', edges='closed', bbox=(1,0, 1, 1)\n",
    "         ,rowColours=sns.color_palette('Reds',10))\n",
    "the_table.auto_set_font_size(False)\n",
    "the_table.set_fontsize(10.5)\n",
    "the_table.scale(2, 2)\n",
    "average_filtered.sort_values('percentage', inplace=True) \n",
    "plt.scatter(average_filtered['percentage'], average_filtered['drug'], color=sns.color_palette('Reds',len(average_filtered)))\n",
    "plt.title('Targets with higher presence in train samples', weight='bold', fontsize=15)\n",
    "plt.xlabel('Percentage', weight='bold')\n",
    "plt.xticks(weight='bold')\n",
    "plt.yticks(weight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> View the high correlation between targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensions Reduction\n",
    "\n",
    "Before running any models and training the data, we are going to clean and remove anything that is not important via Principal Component Analysis(PCA). [Dimensionality Reduction]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.concat([train_features, test_features], ignore_index=True, sort=False)\n",
    "all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_scored=pd.merge(train_features,train_targets_scored,how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove any outliers\n",
    "def cap_outliers(col):\n",
    "    col[col>3]=3\n",
    "    col[col<-3]=-3\n",
    "    return col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtering all the numeric columns\n",
    "numcols=all_data._get_numeric_data().columns\n",
    "all_data_num=all_data.loc[:,numcols]\n",
    "all_data_num=all_data_num.iloc[:,1:]\n",
    "\n",
    "#z=np.abs(stats.zscore(all_data_num['g-0']))\n",
    "#Calculate Z Scores for all the variables. \n",
    "all_data_num=all_data_num.apply(stats.zscore)\n",
    "\n",
    "#Cap the outliers\n",
    "all_data_num=all_data_num.apply(cap_outliers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA function\n",
    "def pca_application(df,n_components,pattern):\n",
    "    df_p=df.loc[:, df.columns.str.startswith(pattern)]\n",
    "    x = StandardScaler().fit_transform(df_p)\n",
    "    pca = PCA(n_components=n_components)\n",
    "    principalComponents = pca.fit_transform(x)\n",
    "    principalDf = pd.DataFrame(data = principalComponents)\n",
    "    return principalDf,pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate principal components separately for GE & CV columns\n",
    "principalDf_g,pca_g=pca_application(all_data,200,'g-')\n",
    "principalDf_c,pca_c=pca_application(all_data,30,'c-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z=np.arange(start=1, stop=len(pca_g.explained_variance_ratio_)+1, step=1)\n",
    "z\n",
    "\n",
    "plt.bar(x=z,height=pca_g.explained_variance_ratio_)\n",
    "plt.xlabel('Principal Components')\n",
    "plt.ylabel('Explained variance')\n",
    "plt.title('Explained Variance for Gene Expression Variable PCAs')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(np.cumsum(pca_g.explained_variance_ratio_))\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance')\n",
    "plt.title('Cumulative Explained Variance for Gene Expression Variable PCAs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z=np.arange(start=1, stop=len(pca_c.explained_variance_ratio_)+1, step=1)\n",
    "z\n",
    "\n",
    "plt.bar(x=z,height=pca_c.explained_variance_ratio_)\n",
    "plt.xlabel('Principal Components')\n",
    "plt.ylabel('Explained variance')\n",
    "plt.title('Explained Variance for Cell Viability Variable PCAs')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(np.cumsum(pca_c.explained_variance_ratio_))\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance')\n",
    "plt.title('Cumulative Explained Variance for Cell Viability Variable PCAs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> These plots show the expanded variance plots and the purpose is to identify where the curve starts to flatten. Overall the cell viability have a high correlation with each other, while the gene expression do not. \n",
    "\n",
    "The aim is then to extract 115 Gene expression components and 15 cell viability. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting the principal components\n",
    "pca_g=principalDf_g.iloc[:,:115]\n",
    "l=[]\n",
    "for i in range(1,116):\n",
    "    var='pca_g'+str(i)\n",
    "    l.append(var)\n",
    "#l\n",
    "pca_g.columns=l\n",
    "pca_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting the principal components\n",
    "pca_c=principalDf_c.iloc[:,:15]\n",
    "#pca_c.columns=['pca_c1','pca_c2','pca_c3','pca_c4','pca_c5']\n",
    "#pca_c\n",
    "l=[]\n",
    "for i in range(1,16):\n",
    "    var='pca_c'+str(i)\n",
    "    l.append(var)\n",
    "#l\n",
    "pca_c.columns=l\n",
    "pca_c\n",
    "\n",
    "#Merging the principal components dataframes\n",
    "pca_cg=pd.merge(pca_c, pca_g, left_index=True, right_index=True)\n",
    "pca_cg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_cols=all_data.iloc[:,1:4]\n",
    "pca_cg_cp=pd.merge(cp_cols, pca_cg, left_index=True, right_index=True)\n",
    "pca_cg_cp['flag']=all_data['flag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating dummies for categorical variables\n",
    "pca_cg_cp=pd.get_dummies(pca_cg_cp, columns=['cp_type', 'cp_dose'])\n",
    "pca_cg_cp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_cg_cp_train=pca_cg_cp.loc[pca_cg_cp['flag']=='train',:]\n",
    "pca_cg_cp_train\n",
    "del pca_cg_cp_train['flag']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=pca_cg_cp_train #Selecting feature variables\n",
    "Y=train_features_scored.iloc[:,-207:-1] #Selecting the output columns\n",
    "feature_list=pca_cg_cp_train.columns\n",
    "\n",
    "#Split data into train and test datasets\n",
    "X_train,X_test,Y_train,Y_test=train_test_split(X, Y,test_size=0.3,random_state=1)\n",
    "\n",
    "def get_tail_label(df: pd.DataFrame, ql=[0.05, 1.]) -> list:\n",
    "    \"\"\"\n",
    "    Find the underrepresented targets.\n",
    "    Underrepresented targets are those which are observed less than the median occurance.\n",
    "    Targets beyond a quantile limit are filtered.\n",
    "    \"\"\"\n",
    "    irlbl = df.sum(axis=0)\n",
    "    irlbl = irlbl[(irlbl > irlbl.quantile(ql[0])) & ((irlbl < irlbl.quantile(ql[1])))]  # Filtering\n",
    "    irlbl = irlbl.max() / irlbl\n",
    "    threshold_irlbl = irlbl.median()\n",
    "    tail_label = irlbl[irlbl > threshold_irlbl].index.tolist()\n",
    "    return tail_label\n",
    "\n",
    "def get_minority_samples(X: pd.DataFrame, y: pd.DataFrame, ql=[0.05, 1.]):\n",
    "    \"\"\"\n",
    "    return\n",
    "    X_sub: pandas.DataFrame, the feature vector minority dataframe\n",
    "    y_sub: pandas.DataFrame, the target vector minority dataframe\n",
    "    \"\"\"\n",
    "    tail_labels = get_tail_label(y, ql=ql)\n",
    "    index = y[y[tail_labels].apply(lambda x: (x == 1).any(), axis=1)].index.tolist()\n",
    "    \n",
    "    X_sub = X[X.index.isin(index)].reset_index(drop = True)\n",
    "    y_sub = y[y.index.isin(index)].reset_index(drop = True)\n",
    "    return X_sub, y_sub\n",
    "\n",
    "def nearest_neighbour(X: pd.DataFrame, neigh) -> list:\n",
    "    \"\"\"\n",
    "    Give index of 10 nearest neighbor of all the instance\n",
    "    \n",
    "    args\n",
    "    X: np.array, array whose nearest neighbor has to find\n",
    "    \n",
    "    return\n",
    "    indices: list of list, index of 5 NN of each element in X\n",
    "    \"\"\"\n",
    "    nbs = NearestNeighbors(n_neighbors=neigh, metric='euclidean', algorithm='kd_tree').fit(X)\n",
    "    euclidean, indices = nbs.kneighbors(X)\n",
    "    return indices\n",
    "\n",
    "\n",
    "def MLSMOTE(X, y, n_sample, neigh=5):\n",
    "    \"\"\"\n",
    "    Give the augmented data using MLSMOTE algorithm\n",
    "    \n",
    "    args\n",
    "    X: pandas.DataFrame, input vector DataFrame\n",
    "    y: pandas.DataFrame, feature vector dataframe\n",
    "    n_sample: int, number of newly generated sample\n",
    "    \n",
    "    return\n",
    "    new_X: pandas.DataFrame, augmented feature vector data\n",
    "    target: pandas.DataFrame, augmented target vector data\n",
    "    \"\"\"\n",
    "    indices2 = nearest_neighbour(X, neigh=5)\n",
    "    n = len(indices2)\n",
    "    new_X = np.zeros((n_sample, X.shape[1]))\n",
    "    target = np.zeros((n_sample, y.shape[1]))\n",
    "    for i in range(n_sample):\n",
    "        reference = random.randint(0, n-1)\n",
    "        neighbor = random.choice(indices2[reference, 1:])\n",
    "        all_point = indices2[reference]\n",
    "        nn_df = y[y.index.isin(all_point)]\n",
    "        ser = nn_df.sum(axis = 0, skipna = True)\n",
    "        target[i] = np.array([1 if val > 0 else 0 for val in ser])\n",
    "        ratio = random.random()\n",
    "        gap = X.loc[reference,:] - X.loc[neighbor,:]\n",
    "        new_X[i] = np.array(X.loc[reference,:] + ratio * gap)\n",
    "    new_X = pd.DataFrame(new_X, columns=X.columns)\n",
    "    target = pd.DataFrame(target, columns=y.columns)\n",
    "    return new_X, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv(\"/Users/mdespinoza/Desktop/Github/data/X_train.csv\")\n",
    "Y_train = pd.read_csv(\"/Users/mdespinoza/Desktop/Github/data/Y_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier(n_estimators=200, max_depth=10, random_state=0,min_samples_split=10)\n",
    "model.fit(X_train,Y_train)#Fitting the model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating predictions from Random Fores Models\n",
    "pred_rf=model.predict(X_test)\n",
    "pred_rf_proba=model.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "feat_importances = pd.Series(model.feature_importances_, index=feature_list)\n",
    "feat_importances=feat_importances.sort_values()\n",
    "feat_importances.plot(kind='barh',figsize=(16,16))#Plotting feature importance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('Model Accuracy')\n",
    "print(model.score(X_test,Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_model= MultiOutputClassifier(LogisticRegression(max_iter=1000, tol=0.1, C = 0.5,verbose=0,random_state = 42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper Functions\n",
    "\n",
    "def Extract(lst): \n",
    "    return [item[:,1] for item in lst] \n",
    "\n",
    "def calc_loss_df(pred_proba):\n",
    "    out=Extract(pred_proba)\n",
    "    arr=np.array(out)\n",
    "    arr1=np.transpose(arr)\n",
    "    l=[]\n",
    "    col=[]\n",
    "    testcols=Y_test.columns\n",
    "    y=np.array(Y_test)\n",
    "    \n",
    "    for i in range(0,206):\n",
    "        a=arr1[:,i].astype('float')\n",
    "        b=y[:,i].astype('int')\n",
    "        if np.sum(b)>0:\n",
    "            l.append(log_loss(b,a))\n",
    "            col.append(testcols[i])\n",
    "\n",
    "    err=pd.DataFrame(\n",
    "        {'cols': col,\n",
    "         'log_loss': l\n",
    "        })\n",
    "    err=err.sort_values(by='log_loss',ascending=False)\n",
    "    return err  \n",
    "\n",
    "def pred_transform(preds):\n",
    "    out=Extract(preds)\n",
    "    arr=np.array(out)\n",
    "    arr1=np.transpose(arr)\n",
    "    return arr1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_rf_proba_t=pred_transform(pred_rf_proba)\n",
    "\n",
    "print(log_loss(np.array(Y_test),np.array(pred_rf_proba_t)))#Compute Log loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating Log Loss by Individual Output Column\n",
    "#y=np.array(Y_test)\n",
    "\n",
    "err=calc_loss_df(pred_rf_proba)\n",
    "\n",
    "err_head=err.head(10)\n",
    "err_tail=err.tail(10)\n",
    "\n",
    "fig2 = px.histogram(err, x=\"log_loss\",opacity=0.6, title='RF - Distribution of Log Loss values on test dataset')\n",
    "fig2.show()\n",
    "\n",
    "fig=px.bar(err_head,x='cols',y='log_loss',title='RF - Output Variables with Highest Log Loss')\n",
    "fig.show()\n",
    "\n",
    "fig1=px.bar(err_tail,x='cols',y='log_loss',title='RF - Output Variables with Lowest Log Loss')\n",
    "fig1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_model= MultiOutputClassifier(LogisticRegression(max_iter=1000, tol=0.1, C = 0.5,verbose=0,random_state = 42))\n",
    "log_model.fit(X_train,Y_train)#Fitting the model \n",
    "#log_model.fit(X_res,y_res)\n",
    "#Generating predictions\n",
    "pred_log_proba=log_model.predict_proba(X_test)\n",
    "pred_log_proba_t=pred_transform(pred_log_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using parameters from https://www.kaggle.com/fchmiel/xgboost-baseline-multilabel-classification\n",
    "xgb = MultiOutputClassifier(XGBClassifier(tree_method='gpu_hist'))\n",
    "\n",
    "params = {'estimator__colsample_bytree': 0.6522,\n",
    "          'estimator__gamma': 3.6975,\n",
    "          'estimator__learning_rate': 0.0503,\n",
    "          'estimator__max_delta_step': 2.0706,\n",
    "          'estimator__max_depth': 10,\n",
    "          'estimator__min_child_weight': 31.5800,\n",
    "          'estimator__n_estimators': 166,\n",
    "          'estimator__subsample': 0.8639\n",
    "         }\n",
    "\n",
    "xgb.set_params(**params)\n",
    "xgb.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_xg_proba = xgb.predict_proba(X_test)\n",
    "df_ll_xg=calc_loss_df(pred_xg_proba)\n",
    "pred_xg_proba_t=pred_transform(pred_xg_proba)\n",
    "\n",
    "print(log_loss(np.array(Y_test),pred_xg_proba_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ll_log=calc_loss_df(pred_log_proba)\n",
    "#print('Log Loss for Logistic Regression',df_ll_log)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Histogram(x=err['log_loss'],name='Random Forest'))\n",
    "fig.add_trace(go.Histogram(x=df_ll_log['log_loss'],name='Logistic Regression'))\n",
    "fig.add_trace(go.Histogram(x=df_ll_xg['log_loss'],name='XGBoost'))\n",
    "\n",
    "fig.update_layout(barmode='overlay',title='Comparison of Distribution of Log Loss values for models')\n",
    "# Reduce opacity to see both histograms\n",
    "fig.update_traces(opacity=0.6)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_actions_r=pd.merge(left=depsum,right=err,how='inner',left_on='action',right_on='cols')\n",
    "loss_actions_r.columns=['sum_actions','action','cols','log_loss_rf']\n",
    "\n",
    "loss_actions_r_x=pd.merge(left=loss_actions_r,right=df_ll_xg,how='inner')\n",
    "loss_actions_r_x.columns=['sum_actions','action','cols','log_loss_rf','log_loss_xg']\n",
    "#loss_actions_r_x\n",
    "\n",
    "loss_actions_r_x_l=pd.merge(left=loss_actions_r_x,right=df_ll_log,how='inner')\n",
    "loss_actions_r_x_l.columns=['sum_actions','action','cols','log_loss_rf','log_loss_xg','log_loss_logistic']\n",
    "loss_actions_r_x_l\n",
    "\n",
    "fig=go.Figure()\n",
    "fig.add_trace(go.Scatter(x=loss_actions_r_x_l['sum_actions'],\n",
    "                        y=loss_actions_r_x_l['log_loss_rf'],mode='markers',name='Random Forest'))\n",
    "fig.add_trace(go.Scatter(x=loss_actions_r_x_l['sum_actions'],\n",
    "                        y=loss_actions_r_x_l['log_loss_xg'],mode='markers',name='XGBoost'))\n",
    "fig.add_trace(go.Scatter(x=loss_actions_r_x_l['sum_actions'],\n",
    "                        y=loss_actions_r_x_l['log_loss_logistic'],mode='markers',name='Logistic Regression'))\n",
    "\n",
    "fig.update_layout(title='Model Performance - Sum of MoAs for Dependent Variable in Train dataset vs Log Loss',xaxis_title='Sum of MoAs Output Variable',yaxis_title='Log Loss value for Output Variable')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion: "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6e5b092f8e2fb83c2095f505bc7ccb0e48374a44d4d066952aa76e49dab04d56"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
